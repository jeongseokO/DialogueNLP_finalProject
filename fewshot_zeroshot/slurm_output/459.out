Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
Traceback (most recent call last):
  File "/home/jeongseokoh/DialogueNLP_finalProject/fewshot_zeroshot/test.py", line 250, in <module>
    main()
  File "/home/jeongseokoh/DialogueNLP_finalProject/fewshot_zeroshot/test.py", line 246, in main
    metrics = evaluate(model, tokenizer, test_dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jeongseokoh/DialogueNLP_finalProject/fewshot_zeroshot/test.py", line 170, in evaluate
    gen_pipe = transformers.pipeline(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 1070, in pipeline
    return pipeline_class(model=model, framework=framework, task=task, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py", line 70, in __init__
    super().__init__(*args, **kwargs)
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py", line 797, in __init__
    self.model.to(device)
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2271, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/jeongseokoh/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

srun: error: gpu-1: task 0: Exited with exit code 1
